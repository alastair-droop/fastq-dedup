#!/usr/bin/env python
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# 
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import print_function

version = '1.2 (2017-11-14)'

import argparse
from sys import stdin, stdout, exit
import json
import logging
from collections import defaultdict
from itertools import combinations

parser = argparse.ArgumentParser(description='Identify possible duplicate FASTQ files from JSON data generated by fastq-id')
parser.add_argument('-v', '--version', action='version', version='%(prog)s {0}'.format(version))
parser.add_argument('-V', '--verbose', dest='verbosity_level', default='warning', choices=['error', 'warning', 'info', 'debug'], help='Set logging level')
parser.add_argument('-c', '--certainty-level', dest='certainty', default='possible', choices=['certain', 'likely', 'possible'], help='certainty level for reporting')
parser.add_argument('-o', '--output-file', dest='output_file', type=argparse.FileType('wt'), default=stdout, help='output file')
parser.add_argument(dest='input_file', type=argparse.FileType('rt'), default=stdin, help='input FASTQ data JSON file file')
args = parser.parse_args()

# A function to exit with logged error message:
def error(message):
    log.error(message)
    exit(1)

# A function to write out duplicate blocks:
def writeDuplicateBlocks(d, dup_type, certainty):
    for i in d[dup_type].keys():
        paths = d[dup_type][i]
        if len(paths) > 1:
            print('{}\t{}\t{}\t{}\t{}'.format(certainty, dup_type, len(paths), i, ', '.join(paths)), file=args.output_file)


# Set up logging based on the verbosity level set by the command line arguments:
log = logging.getLogger()
log_handler = logging.StreamHandler()
log.default_msec_format = ''
log_handler.setFormatter(logging.Formatter('[%(asctime)s.%(msecs)03d] %(levelname)s: %(message)s', '%Y-%m-%d %H:%M:%S'))
log.setLevel(args.verbosity_level.upper())
log.addHandler(log_handler)

# Attempt to read in the complete dataset:
log.info('reading input data...')
try: file_data = json.load(args.input_file)
except (KeyboardInterrupt, SystemExit): exit(0)
except json.decoder.JSONDecodeError as err: error('failed to read input JSON file ({})'.format(str(err)))
except: error('failed to read input JSON file')
log.info('read {} entries from file'.format(len(file_data)))

# Store the duplicte sets:
duplicates = {
    'checksum': defaultdict(list),
    'size_header': defaultdict(list),
    'disk_header': defaultdict(list),
    'header': defaultdict(list)
}

# Iterate through all files grouping together duplicates:
i = 0
for f in file_data:
    try:
        file_path = '{dir}/{name}'.format(**f)
        file_checksum = f['checksum']
        file_header = f['header']
        file_size = f['size']
        file_disk = f['disk']
    except (KeyboardInterrupt, SystemExit): exit(0)
    except KeyError as err:
        log.warning('file entry {} missing required key: {}'.format(i + 1, str(err)))
        continue
    log.debug('file {}: {}'.format(file_path, '(format: {format}; size on disk: {disk}; uncompressed size: {size}; header: {header}; checksum: {checksum})'.format(**f)))
    if file_checksum is not None: duplicates['checksum'][file_checksum].append(file_path)
    if file_header is not None:
        duplicates['header'][file_header].append(file_path)
        if file_size is not None:
            duplicates['size_header']['{} & {}'.format(file_size, file_header)].append(file_path)
        if file_disk is not None:
            duplicates['disk_header']['{} & {}'.format(file_disk, file_header)].append(file_path)
    i += 1
log.info('processed {} files'.format(i))
    
# Iterate through the file data, returning groups:
writeDuplicateBlocks(duplicates, 'checksum', 'CERTAIN_DUPLICATES')
if args.certainty in ['likely', 'possible']:
    writeDuplicateBlocks(duplicates, 'size_header', 'LIKELY_DUPLICATES')
    writeDuplicateBlocks(duplicates, 'disk_header', 'LIKELY_DUPLICATES')
if args.certainty in ['possible']:
    writeDuplicateBlocks(duplicates, 'header', 'POSSIBLE_DUPLICATES')
